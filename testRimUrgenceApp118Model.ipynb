{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9af7919b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bec677b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8674aa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "RimUrgenceAndApp118_model = tf.keras.models.load_model('RimUrgenceAndApp118.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "beb432be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\chouaib\\AppData\\Local\\Temp\\tmpfmjrfx49\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\chouaib\\AppData\\Local\\Temp\\tmpfmjrfx49\\assets\n"
     ]
    }
   ],
   "source": [
    "# Convert the model to model tensorflewlite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(RimUrgenceAndApp118_model)\n",
    "tflite_model = converter.convert()\n",
    "# Save the model\n",
    "with open('RimUrgenceAndApp118.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "84ad2a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'serving_default_conv2d_input:0', 'index': 0, 'shape': array([  1, 224, 224,   3]), 'shape_signature': array([ -1, 224, 224,   3]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "[{'name': 'StatefulPartitionedCall:0', 'index': 17, 'shape': array([1, 2]), 'shape_signature': array([-1,  2]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path='RimUrgenceAndApp118.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "print(interpreter.get_input_details())\n",
    "print(interpreter.get_output_details())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74e08212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file_path):\n",
    "    \n",
    "    # Read in image from file path\n",
    "#     byte_img = tf.io.read_file(file_path)\n",
    "    # Load in the image \n",
    "#     img = tf.io.decode_jpeg(byte_img)\n",
    "    img_array = cv2.imread(file_path)\n",
    "#     crop_image = img_array[x2: x2 + height, x1: x1 + width]\n",
    "    new_img_array = cv2.resize(img_array, (224, 224))\n",
    "    # Preprocessing steps - resizing the image to be 100x100x3\n",
    "#     img = tf.image.resize(byte_img, (224,224))\n",
    "    \n",
    "    img = new_img_array.reshape(-1, 224, 224, 3)\n",
    "    # Scale image to be between 0 and 1 \n",
    "    img = img / 255.0\n",
    "    \n",
    "    # Return image\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "70fe4420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la dimention de l'image (1, 224, 224, 3)\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "le information de la prediction c'est [[1.1930174e-10 1.0000000e+00]]\n",
      " is not fire\n"
     ]
    }
   ],
   "source": [
    "# test1\n",
    "img = preprocess('fire_dataset/non_fire_images/non_fire.34.png')\n",
    "print(f\"la dimention de l'image {img.shape}\")\n",
    "prediction = RimUrgenceAndApp118_model.predict(img)\n",
    "print(f\"le information de la prediction c'est {prediction}\")\n",
    "pred = np.argmax(prediction)\n",
    "if pred == 0:\n",
    "   print(\" is fire\")\n",
    "else:\n",
    "  print(\" is not fire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ad151dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la dimention de l'image (1, 224, 224, 3)\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "le information de la prediction c'est [[3.5310213e-06 9.9999642e-01]]\n",
      " is not fire\n"
     ]
    }
   ],
   "source": [
    "# test2 <2,7,8,12,14,16,57>\n",
    "img2 = preprocess('fire_dataset/fire_images/fire.6.png')\n",
    "print(f\"la dimention de l'image {img2.shape}\")\n",
    "prediction2 = selam_model.predict(img2)\n",
    "print(f\"le information de la prediction c'est {prediction2}\")\n",
    "pred2 = np.argmax(prediction2)\n",
    "if pred2 == 0:\n",
    "   print(\" is fire\")\n",
    "else:\n",
    "  print(\" is not fire\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
